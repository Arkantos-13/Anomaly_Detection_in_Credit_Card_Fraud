{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries for the project.\n",
    "\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv).\n",
    "import numpy as np # linear algebra.\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data which should be placed in the Desktop and the name of the file is 'creditcard.csv'.\n",
    "\n",
    "df = pd.read_csv('Desktop/creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first five rows from out dataset.\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original shape of the dataset.\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The names of the columns of the dataset.\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The description of our dataset. \n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More information about our data for example tha type of it's column.\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if there are any missing data in our dataset, luckily there is any.\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine number of fraud cases in dataset.\n",
    "# valid transaction if Class = 0 and Fraud if Class = 1. \n",
    "\n",
    "\n",
    "valid = len(df[df['Class'] == 0])\n",
    "fraud= len(df[df['Class'] == 1])\n",
    "\n",
    "Outlier_Fraction = (fraud/valid)*100\n",
    "\n",
    "print('OutlierFraction is :' , Outlier_Fraction) \n",
    "print('Valid Transactions:'  , valid ) \n",
    "print('Fraud Transactions:'  , fraud )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about the valid transactions only.\n",
    "\n",
    "print('Amount details of valid transaction')\n",
    "\n",
    "valid_info= df[(df['Class']==0)]\n",
    "valid_info.Amount.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about the fraud transactions only.\n",
    "\n",
    "print('Amount details of fraud transaction')\n",
    "\n",
    "fraud_info = df[df['Class'] ==1]\n",
    "fraud_info.Amount.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Notice how imbalanced is our original dataset. Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud. But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for the fraud and valid trasactions in a graph for better understanding. \n",
    "\n",
    "count_classes = pd.value_counts(df['Class'], sort = True ).sort_index()\n",
    "count_classes.plot(kind = 'bar' ,rot = 0 ,colormap ='plasma')\n",
    "\n",
    "plt.title ( \"Fraud Class histogram\" )\n",
    "plt.xlabel( \"Class\" )\n",
    "plt.ylabel( \"Frequency\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix of the original dataset, we will not use this in order to find our outliers.\n",
    "# We use it only for a quick view and understanding the original dataset. \n",
    "\n",
    "corrmat = df.corr() \n",
    "fig = plt.figure(figsize = (20, 13)) \n",
    "sns.heatmap(corrmat, cmap='viridis', vmax = 1, vmin=-0.5 , square = True , linewidths= 0.05)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of our dataset via Time.\n",
    "\n",
    "sns.kdeplot(df['Time'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of our dataset via Amount of transaction.  \n",
    "\n",
    "sns.kdeplot(df['Amount'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Vi columns are already scaled that's why we only scaled the columns 'Amount' and 'Time'.\n",
    "After this we must remove the old columns and replace them with the new, making a new dataset with the right values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stc = StandardScaler()\n",
    "\n",
    "df['scaled_amount'] = stc.fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "df['scaled_time'] = stc.fit_transform(df['Time'].values.reshape(-1,1))\n",
    "\n",
    "df.drop(['Amount' , 'Time'] , axis = 1 , inplace = True )\n",
    "\n",
    "scaled_amount = df['scaled_amount']\n",
    "scaled_time = df['scaled_time']\n",
    "\n",
    "df.drop(['scaled_amount' , 'scaled_time'] , axis = 1 , inplace = True )\n",
    "\n",
    "df.insert(0 , 'scaled_amount' , scaled_amount)\n",
    "df.insert(1 , 'scaled_time' , scaled_time)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will produce a sub-sample from the previous dataset which will contain randomly 492 valid transactions \n",
    "and 492 fraud transactions.\n",
    "We must do this because in the beginning of this notebook we saw that the original dataframe was heavily imbalanced.\n",
    "By doing this we help our algorithms better understand patterns that determines whether a transaction is a fraud or not \n",
    "which is our target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "\n",
    "fraud_df =  df[df['Class'] ==1]\n",
    "valid_df = df.loc[np.random.choice(df.index, 492, replace=False)]\n",
    "\n",
    "normal_distributed_df = pd.concat([fraud_df, valid_df])\n",
    "\n",
    "# Shuffle dataframe rows\n",
    "new_df = normal_distributed_df.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "new_df = pd.DataFrame(new_df)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our new dataset which contain 984 transactions in random order (492 valid and 492 fraud).\n",
    "\n",
    "normal_distributed_df1 = pd.concat([fraud_df ,valid_df], axis=0)\n",
    "new_df = normal_distributed_df1.sample(frac=1, random_state=42)\n",
    "\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We present the new dataset of the equally possible transactions.\n",
    "\n",
    "print('Distribution of the Classes in the new dataset')\n",
    "\n",
    "sns.countplot('Class', data=new_df)\n",
    "sns.color_palette(\"Set2\", 2)\n",
    "print('\\n')\n",
    "plt.title('Equally Distributed Classes',fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing ratio.\n",
    "# Our equally likely data as we can see below. \n",
    "\n",
    "print(\"Percentage of normal transactions: \", (len(new_df[new_df['Class']==0])/len(new_df)))\n",
    "print(\"Percentage of fraud transactions: \", (len(new_df[new_df['Class']==1])/len(new_df)))\n",
    "print(\"Total number of transactions in resampled data: \", len(new_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will not use this heatmap as a reference.\n",
    "\n",
    "corr = df.corr()\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "sns.heatmap(corr, cmap='viridis', annot_kws={'size':20},linewidths= 0.05)\n",
    "ax.set_title(\"Old Correlation Matrix \\n (don't use for reference)\", fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use this heatmap as a reference.\n",
    "\n",
    "corr1 = new_df.corr()\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax.set_title('SubSample Correlation Matrix \\n (use for reference)', fontsize=13)\n",
    "sns.heatmap(corr, cmap='viridis', annot_kws={'size':20}, linewidths= 0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the positive correletaion which can be our outliers.\n",
    "# Positive correlation: The higher the feature value the probability increases that it will be a fraudulent transaction.\n",
    "\n",
    "f, axes = plt.subplots(ncols=5, figsize=(25,15))\n",
    "\n",
    "sns.boxplot(x='Class' , y ='V3',  data = new_df, ax=axes[0])\n",
    "sns.boxplot(x='Class' , y ='V7',  data = new_df, ax=axes[1])\n",
    "sns.boxplot(x='Class' , y ='V14', data = new_df, ax=axes[2])\n",
    "sns.boxplot(x='Class' , y ='V17', data = new_df, ax=axes[3])\n",
    "sns.boxplot(x='Class' , y ='V20', data = new_df, ax=axes[4])\n",
    "\n",
    "print('Positive Correlation Boxtplos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IQR METHOD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE OF THE EXTREME OUTLIERS FROM TOP 2 POSITIVE CORRELATION \n",
    "# FIRST THE V20 \n",
    "\n",
    "v20_fraud = new_df['V20'].loc[new_df['Class'] == 1 ].values\n",
    "\n",
    "q25 = np.percentile(v20_fraud , 25)\n",
    "q75 = np.percentile(v20_fraud , 75)\n",
    "\n",
    "print('The 25th Quartile is :' , q25)\n",
    "print('The 75th Quantile is :' , q75)\n",
    "print('\\n')\n",
    "\n",
    "v20_iqr = q75 - q25\n",
    "\n",
    "print('The IQR of V20 is :' , v20_iqr)\n",
    "\n",
    "v20_off = 1.5 * v20_iqr\n",
    "v20_lower = q25 - v20_off\n",
    "v20_upper = q75 + v20_off\n",
    "\n",
    "print('The v20 that we will remove is :' , v20_off)\n",
    "print('The min point is :' , v20_lower)\n",
    "print('The max point is :' , v20_upper)\n",
    "print('\\n')\n",
    "\n",
    "outliers_v20 = [i for i in v20_fraud if i < v20_lower or i > v20_upper ]\n",
    "new_df_v20 = new_df.drop(new_df[(new_df['V20'] > v20_upper) | (new_df['V20'] < v20_lower)].index)\n",
    "\n",
    "print('The number of the outliers is : ', len(outliers_v20))\n",
    "print('The number of transactions after the outliers removes is :' , len(new_df_v20))\n",
    "print('The new dataset after we remove the outliers of v20 is :' , new_df_v20.shape)\n",
    "\n",
    "#984 - 41 = 943 , so our model applies good so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can compare our boxplots before and after we remove the outliers of V20. \n",
    "\n",
    "f,(ax1, ax2) = plt.subplots(1, 2, figsize=(10,10))\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V20\", data=new_df, ax=ax1)\n",
    "sns.boxplot(x=\"Class\", y=\"V20\", data=new_df_v20, ax=ax2)\n",
    "\n",
    "print('Before and After we apply the IQR method and remove the outliers of V20')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE OF THE EXTREME OUTLIERS FROM TOP 2 POSITIVE CORRELATION \n",
    "# FIRST THE V3\n",
    "\n",
    "v3_fraud = new_df['V3'].loc[new_df['Class'] == 1 ].values\n",
    "\n",
    "q25 = np.percentile(v3_fraud , 25)\n",
    "q75 = np.percentile(v3_fraud , 75)\n",
    "\n",
    "print('The 25th Quartile is :' , q25)\n",
    "print('The 75th Quantile is : ' , q75)\n",
    "print('\\n')\n",
    "\n",
    "v3_iqr = q75 - q25\n",
    "\n",
    "print('The IQR of V3 is :' , v3_iqr)\n",
    "print('\\n')\n",
    "v3_off = 1.5 * v3_iqr\n",
    "v3_lower = q25 - v3_off\n",
    "v3_upper = q75 + v3_off\n",
    "\n",
    "print('The v3 tha we will remove is :' , v3_off)\n",
    "print('The min point is :' , v3_lower)\n",
    "print('The max point is :' , v3_upper)\n",
    "print('\\n')\n",
    "\n",
    "outliers_v3 = [i for i in v3_fraud if i < v3_lower or i > v3_upper ]\n",
    "new_df_v3 = new_df.drop(new_df[(new_df['V3'] > v3_upper) | (new_df['V3'] < v3_lower)].index)\n",
    "\n",
    "print('The number of the outliers is : ', len(outliers_v3))\n",
    "print('The number of transactions after the outliers removes is :' , len(new_df_v3))\n",
    "print('The new dataset after we remove the outliers of v3 is :' ,new_df_v3.shape)\n",
    "\n",
    "#984 - 53 = 931 , so our model is really good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can compare our boxplots before and after we remove the outliers of V3 \n",
    "\n",
    "f,(ax1, ax2) = plt.subplots(1, 2, figsize=(10,10))\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V3\", data=new_df,ax=ax1)\n",
    "sns.boxplot(x=\"Class\", y=\"V3\", data=new_df_v3,ax=ax2)\n",
    "\n",
    "print('Before and After we apply the IQR method and remove the outliers of V3')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative correlation: The lower the feature value,the probability decreases that it will be a fraudulent transaction.\n",
    "\n",
    "f, axes = plt.subplots(ncols=5, figsize=(25,10))\n",
    "\n",
    "sns.boxplot(x='Class' , y ='V10', data = new_df, ax=axes[0])\n",
    "sns.boxplot(x='Class' , y ='V12', data = new_df, ax=axes[1])\n",
    "sns.boxplot(x='Class' , y ='V14', data = new_df, ax=axes[2])\n",
    "sns.boxplot(x='Class' , y ='V16', data = new_df, ax=axes[3])\n",
    "sns.boxplot(x='Class' , y ='V17', data = new_df, ax=axes[4])\n",
    "\n",
    "print('Negative Correlation Boxtplos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# REMOVE OF THE EXTREME OUTLIERS FROM TOP 2 NEGATIVE CORRELATION \n",
    "# FIRST THE V10\n",
    "\n",
    "v10_fraud = new_df['V10'].loc[new_df['Class'] == 1 ].values\n",
    "\n",
    "q25 = np.percentile(v10_fraud , 25)\n",
    "q75 = np.percentile(v10_fraud , 75)\n",
    "\n",
    "print('The 25th Quartile is :' , q25)\n",
    "print('The 75th Quantile is :' , q75)\n",
    "print('\\n')\n",
    "\n",
    "v10_iqr = q75 - q25\n",
    "\n",
    "print('The IQR of V10 is :' , v10_iqr)\n",
    "\n",
    "v10_off = 1.5* v10_iqr\n",
    "v10_lower = q25 - v10_off\n",
    "v10_upper = q75 + v10_off\n",
    "\n",
    "print('The v10 tha we will remove is :' , v10_off)\n",
    "print('The min point is :' , v10_lower)\n",
    "print('The max point is :' , v10_upper)\n",
    "print('\\n')\n",
    "\n",
    "outliers_v10 = [i for i in v10_fraud if i < v10_lower or i > v10_upper ]\n",
    "new_df_v10 = new_df.drop(new_df[(new_df['V10'] > v10_upper) | (new_df['V10'] < v10_lower)].index)\n",
    "\n",
    "print('The number of the outliers is : ', len(outliers_v10))\n",
    "print('The number of transactions after the outliers removes is :' , len(new_df_v10))\n",
    "print('The new dataset after we remove the outliers of v10 is :' ,new_df_v10.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can compare our boxplots before and after we remove the outliers of V10\n",
    "\n",
    "f,(ax1, ax2) = plt.subplots(1, 2, figsize=(13,10))\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V10\", data=new_df, ax = ax1 )\n",
    "sns.boxplot(x=\"Class\", y=\"V10\", data=new_df_v10, ax = ax2 )\n",
    "\n",
    "print('Before and After we apply the IQR method and remove the outliers of V10')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# REMOVE OF THE EXTREME OUTLIERS FROM TOP 2 NEGATIVE CORRELATION \n",
    "# FIRST THE V10\n",
    "\n",
    "v14_fraud = new_df['V14'].loc[new_df['Class'] == 1 ].values\n",
    "\n",
    "q25 = np.percentile(v14_fraud , 25)\n",
    "q75 = np.percentile(v14_fraud , 75)\n",
    "\n",
    "print('The 25th Quartile is :' , q25)\n",
    "print('The 75th Quantile is : ' , q75)\n",
    "print('\\n')\n",
    "\n",
    "v14_iqr = q75 - q25\n",
    "\n",
    "print('The IQR of V14 is :' , v14_iqr)\n",
    "\n",
    "v14_off = 1.5* v14_iqr\n",
    "v14_lower = q25 - v14_off\n",
    "v14_upper = q75 + v14_off\n",
    "\n",
    "print('The v14 tha we will remove is :' , v14_off)\n",
    "print('The min point is :' , v14_lower)\n",
    "print('The max point is :' , v14_upper)\n",
    "print('\\n')\n",
    "\n",
    "outliers_v14 = [i for i in v14_fraud if i < v14_lower or i > v14_upper ]\n",
    "new_df_v14 = new_df.drop(new_df[(new_df['V14'] > v14_upper) | (new_df['V14'] < v14_lower)].index)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('The number of the outliers is : ', len(outliers_v14))\n",
    "print('The number of transactions after the outliers removes is :' , len(new_df_v14))\n",
    "print('The new dataset after we remove the outliers of v14 is :' ,new_df_v14.shape)\n",
    "\n",
    "#984 - 5 = 979 , so our model is real good "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can compare our boxplots before and after we remove the outliers of V14\n",
    "\n",
    "f,(ax1, ax2) = plt.subplots(1, 2, figsize=(10,10))\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V14\", data=new_df,ax=ax1)\n",
    "sns.boxplot(x=\"Class\", y=\"V14\", data=new_df_v14,ax=ax2)\n",
    "\n",
    "print('Before and After we apply the IQR method and remove the outliers of V14')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Class'], axis=1)\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the dataset in train set and test set, using sklearn libraly.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (confusion_matrix, precision_recall_curve, classification_report, \n",
    "                            precision_score, recall_score, accuracy_score,f1_score,roc_auc_score)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISOLATION FOREST TREE MODEL\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "isf = IsolationForest(max_samples = len(X)).fit(X) # Fitting the model.\n",
    "y_prediction = isf.predict(X)                      # Prediction using trained model.\n",
    "\n",
    "# The isolation forest use (-1,1) in order to predict the result.\n",
    "# But we have valid = 0 and fraud = 1 , so we must make some changes  before we run it. \n",
    "\n",
    "y_prediction[y_prediction == 1] = 0                # Valid transactions are labelled as 0.\n",
    "y_prediction[y_prediction == -1] = 1               # Fraudulent transactions are labelled as 1.\n",
    "\n",
    "errors = (y_prediction != y).sum()                 # Total number of errors is calculated.\n",
    "\n",
    "print('The errors of the Isolation Forest model is ', errors)\n",
    "\n",
    "print('\\n')\n",
    "print(\"Model Accuracy:\", round(accuracy_score(y_prediction , y),2))\n",
    "print(\"Model Precision:\", round(precision_score(y_prediction , y),2))\n",
    "print(\"Model Recall:\", round(recall_score(y_prediction , y),2))\n",
    "print(\"Model F1-Score:\", round(f1_score(y_prediction , y),2))\n",
    "print(\"Model ROC:\", round(roc_auc_score(y_prediction , y),2))\n",
    "\n",
    "\n",
    "print(classification_report(y_prediction , y))\n",
    "\n",
    "\n",
    "conf_matrix=confusion_matrix(y_prediction,y )\n",
    "print('\\n')\n",
    "labels= ['Valid', 'Fraud'] \n",
    "plt.figure(figsize=(6, 6)) \n",
    "sns.heatmap(pd.DataFrame(conf_matrix), xticklabels= labels, yticklabels= labels, \n",
    "            linewidths= 0.05 ,annot=True, fmt=\"d\" , cmap='BuPu')\n",
    "\n",
    "plt.title(\"Isolation Forest Classifier - Confusion Matrix\") \n",
    "plt.ylabel('True Value') \n",
    "plt.xlabel('Predicted Value') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM_FOREST_CLASSIFIER MODEL \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rfc=RandomForestClassifier(random_state = 42) \n",
    "rfc.fit(X_train, y_train) \n",
    "Y_pred=rfc.predict(X_test) \n",
    "\n",
    "\n",
    "print(\"Model Accuracy:\", round(accuracy_score(y_test, Y_pred),2))\n",
    "print(\"Model Precision:\", round(precision_score(y_test, Y_pred),2))\n",
    "print(\"Model Recall:\", round(recall_score(y_test, Y_pred),2))\n",
    "print(\"Model F1-Score:\", round(f1_score(y_test, Y_pred),2))\n",
    "print(\"Model ROC:\", round(roc_auc_score(y_test, Y_pred),2))\n",
    "\n",
    "conf_matrix=confusion_matrix(y_test, Y_pred) \n",
    "labels= ['Valid', 'Fraud'] \n",
    "plt.figure(figsize=(6, 6)) \n",
    "\n",
    "sns.heatmap(pd.DataFrame(conf_matrix), xticklabels= labels, yticklabels= labels, \n",
    "            linewidths= 0.05 ,annot=True, fmt=\"d\" , cmap='BuPu')\n",
    "\n",
    "print(classification_report(y_test, Y_pred, target_names=labels))\n",
    "\n",
    "\n",
    "plt.title(\"Random Forest Classifier - Confusion Matrix\") \n",
    "plt.ylabel('True Value') \n",
    "plt.xlabel('Predicted Value') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGISTIC_REGRESSION MODEL\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(random_state = 42)\n",
    "logreg.fit(X_train, y_train)\n",
    "Y_pred1 = logreg.predict(X_test)\n",
    "\n",
    "print(\"Model Accuracy:\", round(accuracy_score(y_test, Y_pred1),2))\n",
    "print(\"Model Precision:\", round(precision_score(y_test, Y_pred1),2))\n",
    "print(\"Model Recall:\", round(recall_score(y_test, Y_pred1),2))\n",
    "print(\"Model F1-Score:\", round(f1_score(y_test, Y_pred1),2))\n",
    "\n",
    "\n",
    "conf_matrix1 = confusion_matrix(y_test, Y_pred1)\n",
    "plt.figure(figsize=(6, 6)) \n",
    "labels= ['Valid', 'Fraud'] \n",
    "\n",
    "sns.heatmap(pd.DataFrame(conf_matrix1),annot=True, fmt='d',\n",
    "            linewidths= 0.05 ,cmap='BuPu',xticklabels= labels, yticklabels= labels)\n",
    "\n",
    "print(classification_report(y_test, Y_pred1, target_names=labels))\n",
    "\n",
    "plt.title('Logistic Regression - Confusion Matrix')\n",
    "plt.ylabel('True Value')\n",
    "plt.xlabel('Predicted Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECISSION TREE CLASSIFIER MODEL\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state = 42)\n",
    "dtc.fit(X_train,y_train)\n",
    "Y_pred2 = dtc.predict(X_test)\n",
    "conf_matrix2 = confusion_matrix(y_test , Y_pred2)\n",
    "\n",
    "print(\"Model Accuracy:\", round(accuracy_score(y_test, Y_pred2),2))\n",
    "print(\"Model Precision:\", round(precision_score(y_test, Y_pred2),2))\n",
    "print(\"Model Recall:\", round(recall_score(y_test, Y_pred2),2))\n",
    "print(\"Model F1-Score:\", round(f1_score(y_test, Y_pred2),2))\n",
    "\n",
    "conf_matrix2 = confusion_matrix(y_test, Y_pred2)\n",
    "plt.figure(figsize=(6, 6))\n",
    "labels= ['Valid', 'Fraud'] \n",
    "\n",
    "sns.heatmap(pd.DataFrame(conf_matrix2),annot=True, fmt='d',linewidths= 0.05 ,cmap='BuPu',\n",
    "            xticklabels= labels, yticklabels= labels)\n",
    "\n",
    "print(classification_report(y_test,Y_pred2,target_names=labels))\n",
    "\n",
    "plt.title('Decission Tree Classifier - Confusion Matrix')\n",
    "plt.ylabel('True Value') \n",
    "plt.xlabel('Predicted Value') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAIVE BAYES CLASSIFIER\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "NB = BernoulliNB()\n",
    "NB.fit(X_train,y_train)\n",
    "Y_pred3 = NB.predict(X_test)\n",
    "conf_matrix_nb = confusion_matrix(y_test , Y_pred3)\n",
    "\n",
    "print(\"Model Accuracy:\", round(accuracy_score(y_test, Y_pred3),2))\n",
    "print(\"Model Precision:\", round(precision_score(y_test, Y_pred3),2))\n",
    "print(\"Model Recall:\", round(recall_score(y_test, Y_pred3),2))\n",
    "print(\"Model F1-Score:\", round(f1_score(y_test, Y_pred3),2))\n",
    "\n",
    "conf_matrix2 = confusion_matrix(y_test, Y_pred3)\n",
    "plt.figure(figsize=(6, 6))\n",
    "labels= ['Valid', 'Fraud'] \n",
    "\n",
    "sns.heatmap(pd.DataFrame(conf_matrix2),annot=True, fmt='d',linewidths= 0.05 ,cmap='BuPu',\n",
    "            xticklabels= labels, yticklabels= labels)\n",
    "\n",
    "print(classification_report(y_test,Y_pred2,target_names=labels))\n",
    "\n",
    "plt.title('Naive Bayes Classifier - Confusion Matrix')\n",
    "plt.ylabel('True Value') \n",
    "plt.xlabel('Predicted Value') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC CLASSIFIER MODEL\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_clf = SVC()\n",
    "svc_clf.fit(X_train,y_train)\n",
    "\n",
    "Y_pred4 = svc_clf.predict(X_test)\n",
    "\n",
    "conf_matrix_svm = confusion_matrix(y_test,Y_pred4)\n",
    "\n",
    "print(\"Model Accuracy:\", round(accuracy_score(y_test, Y_pred4),2))\n",
    "print(\"Model Precision:\", round(precision_score(y_test, Y_pred4),2))\n",
    "print(\"Model Recall:\", round(recall_score(y_test,Y_pred4),2))\n",
    "print(\"Model F1-Score:\", round(f1_score(y_test, Y_pred4),2))\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "labels= ['Valid', 'Fraud'] \n",
    "\n",
    "sns.heatmap(pd.DataFrame(conf_matrix2),annot=True, fmt='d',linewidths= 0.05 ,cmap='BuPu',\n",
    "            xticklabels= labels, yticklabels= labels)\n",
    "\n",
    "print(classification_report(y_test,Y_pred2,target_names=labels))\n",
    "\n",
    "plt.title('SVC Classifier - Confusion Matrix')\n",
    "plt.ylabel('True Value') \n",
    "plt.xlabel('Predicted Value') \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
